#!/usr/bin/env python2
# accession_analysis 0.0.1
# Generated by dx-app-wizard.
#
# Basic execution pattern: Your app will run on a single machine from
# beginning to end.
#
# See https://wiki.dnanexus.com/Developer-Portal for documentation and
# tutorials on how to modify this file.
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import os
import subprocess
import logging
import re
import urlparse
import time
import pprint
import csv
from base64 import b64encode

import dxpy
import common

# logging.getLogger("requests").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)
logger.addHandler(dxpy.DXLogHandler())
logger.propagate = False

common_metadata = {
    'lab': 'encode-processing-pipeline',
    'award': 'U41HG006992'
}

DEPRECATED = ['deleted', 'replaced', 'revoked']


def flat(l):
    result = []
    for el in l:
        if hasattr(el, "__iter__") and not isinstance(el, basestring):
            result.extend(flat(el))
        else:
            result.append(el)
    return result


def dup_parse(dxlink):
    desc = dxpy.describe(dxlink)
    with dxpy.DXFile(desc['id'], mode='r') as dup_file:
        if not dup_file:
            return None

        lines = iter(dup_file.read().splitlines())

        for line in lines:
            if line.startswith('## METRICS CLASS'):
                headers = lines.next().rstrip('\n').lower()
                metrics = lines.next().rstrip('\n')
                break

        headers = headers.split('\t')
        metrics = metrics.split('\t')
        headers.pop(0)
        metrics.pop(0)

        dup_qc = dict(zip(headers, metrics))
    return dup_qc


def xcor_parse(dxlink):
    desc = dxpy.describe(dxlink)
    with dxpy.DXFile(desc['id'], mode='r') as xcor_file:
        if not xcor_file:
            return None

        lines = xcor_file.read().splitlines()
        line = lines[0].rstrip('\n')
        # CC_SCORE FILE format:
        #   Filename <tab>
        #   numReads <tab>
        #   estFragLen <tab>
        #   corr_estFragLen <tab>
        #   PhantomPeak <tab>
        #   corr_phantomPeak <tab>
        #   argmin_corr <tab>
        #   min_corr <tab>
        #   phantomPeakCoef <tab>
        #   relPhantomPeakCoef <tab>
        #   QualityTag

        headers = ['Filename',
                   'numReads',
                   'estFragLen',
                   'corr_estFragLen',
                   'PhantomPeak',
                   'corr_phantomPeak',
                   'argmin_corr',
                   'min_corr',
                   'phantomPeakCoef',
                   'relPhantomPeakCoef',
                   'QualityTag']
        metrics = line.split('\t')
        headers.pop(0)
        metrics.pop(0)

        xcor_qc = dict(zip(headers, metrics))
    return xcor_qc


def pbc_parse(dxlink):
    desc = dxpy.describe(dxlink)
    with dxpy.DXFile(desc['id'], mode='r') as pbc_file:
        if not pbc_file:
            return None

        lines = pbc_file.read().splitlines()
        line = lines[0].rstrip('\n')
        # PBC File output:
        #   TotalReadPairs <tab>
        #   DistinctReadPairs <tab>
        #   OneReadPair <tab>
        #   TwoReadPairs <tab>
        #   NRF=Distinct/Total <tab>
        #   PBC1=OnePair/Distinct <tab>
        #   PBC2=OnePair/TwoPair

        headers = ['TotalReadPairs',
                   'DistinctReadPairs',
                   'OneReadPair',
                   'TwoReadPairs',
                   'NRF',
                   'PBC1',
                   'PBC2']
        metrics = line.split('\t')

        pbc_qc = dict(zip(headers, metrics))
    return pbc_qc


def flagstat_parse(dxlink):
    desc = dxpy.describe(dxlink)
    with dxpy.DXFile(desc['id'], mode='r') as flagstat_file:
        if not flagstat_file:
            return None

    qc_dict = {
        # values are regular expressions,
        # will be replaced with scores [hiq, lowq]
        'in_total': 'in total',
        'duplicates': 'duplicates',
        'mapped': 'mapped',
        'paired_in_sequencing': 'paired in sequencing',
        'read1': 'read1',
        'read2': 'read2',
        'properly_paired': 'properly paired',
        'with_self_mate_mapped': 'with itself and mate mapped',
        'singletons': 'singletons',
        # i.e. at the end of the line
        'mate_mapped_different_chr': 'with mate mapped to a different chr$',
        # RE so must escape
        'mate_mapped_different_chr_hiQ':
            'with mate mapped to a different chr \(mapQ>=5\)'
    }
    flagstat_lines = flagstat_file.read().splitlines()
    for (qc_key, qc_pattern) in qc_dict.items():
        qc_metrics = next(re.split(qc_pattern, line)
                          for line in flagstat_lines
                          if re.search(qc_pattern, line))
        (hiq, lowq) = qc_metrics[0].split(' + ')
        qc_dict[qc_key] = [int(hiq.rstrip()), int(lowq.rstrip())]

    return qc_dict


def get_attachment(dxlink):
    desc = dxpy.describe(dxlink)
    filename = desc['name']
    mime_type = desc['media']
    if mime_type == 'text/plain' and not filename.endswith(".txt"):
        filename += ".txt"
    with dxpy.DXFile(desc['id'], mode='r') as stream:
        obj = {
            'download': filename,
            'type': mime_type,
            'href': 'data:%s;base64,%s' % (mime_type, b64encode(stream.read()))
        }
    return obj


# these are stopgaps until proper QC metrics are available
# and mapping_report uses them.

def qc(stages):
    raw_mapping_stage = stages['Map ENCSR*']['stage_metadata']
    return flagstat_parse(raw_mapping_stage['output']['mapping_statistics'])


def dup_qc(stages):
    qc_stage = stages['Filter and QC*']['stage_metadata']
    return dup_parse(qc_stage['output']['dup_file_qc'])


def pbc_qc(stages):
    qc_stage = stages['Filter and QC*']['stage_metadata']
    return pbc_parse(qc_stage['output']['pbc_file_qc'])


def filtered_qc(stages):
    qc_stage = stages['Filter and QC*']['stage_metadata']
    return flagstat_parse(qc_stage['output']['filtered_mapstats'])


def xcor_qc(stages):
    xcor_stage = stages['Calculate cross-correlation*']['stage_metadata']
    return xcor_parse(xcor_stage['output']['CC_scores_file'])


def chipseq_filter_quality_metric(step_run, stages, files):
    # this is currently a mix of deduplication and cross-correlation stats
    # maybe break out all the xcor stuff to its own object
    logger.debug("in chip_seq_filter_quality_metric with \
        step_run %s stages.keys() %s output files %s"
                 % (step_run, stages.keys(), files))

    file_accessions = list(set(flat([
        resolve_name_to_accessions(stages, output_name)
        for output_name in files])))

    qc_stage = stages['Filter and QC*']['stage_metadata']
    xcor_stage = stages['Calculate cross-correlation*']['stage_metadata']

    xcor_plot = get_attachment(xcor_stage['output']['CC_plot_file'])
    xcor_scores = get_attachment(xcor_stage['output']['CC_scores_file'])

    pbc_qc = pbc_parse(qc_stage['output']['pbc_file_qc'])
    xcor_qc = xcor_parse(xcor_stage['output']['CC_scores_file'])

    obj = {
        'assay_term_id': 'OBI:0000716',
        'assay_term_name': 'ChIP-seq',
        'step_run': step_run,
        'quality_metric_of': file_accessions,
        'cross_correlation_plot': xcor_plot,
        'attachment': xcor_scores,
        'NSC': float(xcor_qc['phantomPeakCoef']),
        'RSC': float(xcor_qc['relPhantomPeakCoef']),
        'fragment length': int(xcor_qc['estFragLen']),
        'PBC1': float(pbc_qc['PBC1']),
        'PBC2': float(pbc_qc['PBC2']),
        'NRF': float(pbc_qc['NRF'])
    }
    # for very small, very complex fastq's, there will be exactly no overlap
    # in such cases, PBC2 will be infinite, but there is no infinity in JSON,
    # so we change that to the string "Infinity"
    if obj['PBC2'] == float('inf'):
        obj['PBC2'] = 'Infinity'
    return [obj]


def samtools_flagstats_quality_metric(step_run, stages, files):
    logger.debug("in chip_seq_filter_quality_metric with \
        step_run %s stages.keys() %s output files %s"
                 % (step_run, stages.keys(), files))

    file_accessions = list(set(flat([
        resolve_name_to_accessions(stages, output_name)
        for output_name in files])))

    qc_stage = stages['Filter and QC*']['stage_metadata']

    flagstat_qc = flagstat_parse(qc_stage['output']['filtered_mapstats'])

    obj = {
        'assay_term_id': 'OBI:0000716',
        'assay_term_name': 'ChIP-seq',
        'step_run': step_run,
        'quality_metric_of': file_accessions,
        'attachment': get_attachment(qc_stage['output']['filtered_mapstats']),
        'total':                int(flagstat_qc['in_total'][0]),
        'total_qc_failed':      int(flagstat_qc['in_total'][1]),
        'duplicates':           int(flagstat_qc['duplicates'][0]),
        'duplicates_qc_failed': int(flagstat_qc['duplicates'][1]),
        'mapped':               int(flagstat_qc['mapped'][0]),
        'mapped_qc_failed':     int(flagstat_qc['mapped'][1]),
        'mapped_pct':           '{:.2%}'.format(
            float(flagstat_qc['mapped'][0]) /
            float(flagstat_qc['in_total'][0]))
    }
    if int(flagstat_qc['paired_in_sequencing'][0]) or \
       int(flagstat_qc['paired_in_sequencing'][1]):
        obj.update({
            'paired':
                int(flagstat_qc['paired_in_sequencing'][0]),

            'paired_qc_failed':
                int(flagstat_qc['paired_in_sequencing'][1]),

            'read1':
                int(flagstat_qc['read1'][0]),

            'read1_qc_failed':
                int(flagstat_qc['read1'][1]),

            'read2':
                int(flagstat_qc['read2'][0]),

            'read2_qc_failed':
                int(flagstat_qc['read2'][1]),

            'paired_properly':
                int(flagstat_qc['properly_paired'][0]),

            'paired_properly_qc_failed':
                int(flagstat_qc['properly_paired'][1]),

            'paired_properly_pct':
                '{:.2%}'.format(
                    float(flagstat_qc['properly_paired'][0]) /
                    float(flagstat_qc['in_total'][0])),

            'with_itself':
                int(flagstat_qc['with_self_mate_mapped'][0]),

            'with_itself_qc_failed':
                int(flagstat_qc['with_self_mate_mapped'][1]),

            'singletons':
                int(flagstat_qc['singletons'][0]),

            'singletons_qc_failed':
                int(flagstat_qc['singletons'][1]),

            'singletons_pct':   '{:.2%}'.format(
                float(flagstat_qc['singletons'][0]) /
                float(flagstat_qc['in_total'][0])),

            'diff_chroms':
                int(flagstat_qc['mate_mapped_different_chr_hiQ'][0]),

            'diff_chroms_qc_failed':
                int(flagstat_qc['mate_mapped_different_chr_hiQ'][1])
        })

    return [obj]


def idr_quality_metric(step_run, stages, files):

    logger.debug("in idr_seq_filter_quality_metric with \
        step_run %s stages.keys() %s output files %s"
                 % (step_run, stages.keys(), files))

    file_accessions = list(set(flat([
        resolve_name_to_accessions(stages, output_name)
        for output_name in files])))

    final_idr_stage_output = \
        stages['Final IDR peak calls']['stage_metadata']['output']

    def IDR_plot(stage_name):
        return get_attachment(
            stages[stage_name]['stage_metadata']['output']['IDR2_plot'])

    def IDR_params(stage_name):
        return get_attachment(
            stages[stage_name]['stage_metadata']['output']['EM_parameters_log'])

    def IDR_threshold(stage_name):
        return float(stages[stage_name]['stage_metadata']['originalInput']['idr_threshold'])

    obj = {
        'assay_term_id':     'OBI:0000716',
        'assay_term_name':   'ChIP-seq',
        'step_run':          step_run,
        'quality_metric_of': file_accessions,

        'Nt': int(final_idr_stage_output['Nt']),
        'Np': int(final_idr_stage_output['Np']),
        'N1': int(final_idr_stage_output['N1']),
        'N2': int(final_idr_stage_output['N2']),

        'self_consistency_ratio':
            float(final_idr_stage_output['self_consistency_ratio']),
        'rescue_ratio':
            float(final_idr_stage_output['rescue_ratio']),
        'reproducibility_test':
            str(final_idr_stage_output['reproducibility_test']),

        'IDR_plot_true':    IDR_plot('IDR True Replicates'),
        'IDR_plot_rep1_pr': IDR_plot('IDR Rep 1 Self-pseudoreplicates'),
        'IDR_plot_rep2_pr': IDR_plot('IDR Rep 2 Self-pseudoreplicates'),
        'IDR_plot_pool_pr': IDR_plot('IDR Pooled Pseudoreplicates'),

        'IDR_parameters_true':    IDR_params('IDR True Replicates'),
        'IDR_parameters_rep1_pr': IDR_params('IDR Rep 1 Self-pseudoreplicates'),
        'IDR_parameters_rep2_pr': IDR_params('IDR Rep 2 Self-pseudoreplicates'),
        'IDR_parameters_pool_pr': IDR_params('IDR Pooled Pseudoreplicates')
    }

    # these were not surfaced as outputs in earlier versions of the
    # ENCODE IDR applet, so need to check first if they're there
    if 'No' in final_idr_stage_output:
        obj.update({'N_optimal': final_idr_stage_output['No']})
    if 'Nc' in final_idr_stage_output:
        obj.update({'N_conservative': final_idr_stage_output['Nc']})

    # IDR cutoff should be the same for all IDR stages
    idr_cutoffs = \
        [IDR_threshold(stage_name) for stage_name in
            ['IDR True Replicates', 'IDR Rep 1 Self-pseudoreplicates',
             'IDR Rep 2 Self-pseudoreplicates', 'IDR Pooled Pseudoreplicates']]
    if all(x == idr_cutoffs[0] for x in idr_cutoffs):
        obj.update({'IDR_cutoff': idr_cutoffs[0]})
    else:  # this is a serious enough error to block creation of the object
        logger.error('Unequal IDR cutoffs: %s' % (idr_cutoffs))
        return None

    return [obj]


def get_rep_bams(experiment, assembly, keypair, server):
    logger.debug('in get_rep_bams with experiment[accession] %s'
                 % (experiment.get('accession')))
    original_files = [common.encoded_get(
        urlparse.urljoin(server, '%s' % (uri)), keypair)
        for uri in experiment.get('original_files')]

    # resolve the biorep_n for each fastq
    for fastq in [f for f in original_files
                  if f.get('file_format') == 'fastq']:
        replicate = common.encoded_get(
            urlparse.urljoin(server, '%s' % (fastq.get('replicate'))), keypair)
        fastq.update(
            {'biorep_n': replicate.get('biological_replicate_number')})

    # resolve the biorep_n's from derived_from for each bam
    for bam in [f for f in original_files
                if f.get('file_format') == 'bam' and
                f.get('assembly') == assembly]:

        biorep_ns = set()

        for derived_from_uri in bam.get('derived_from'):

            # this assumes frame=object
            derived_from_accession = os.path.basename(
                derived_from_uri.strip('/'))

            biorep_ns.add(next(
                f.get('biorep_n')
                for f in original_files
                if f.get('accession') == derived_from_accession))

        if len(biorep_ns) != 1:
            logger.error("%s %s expected 1 biorep_n, found %d, skipping."
                         % (experiment.get('accession'), bam.get('accession')))
            return

        else:
            biorep_n = biorep_ns.pop()
            bam.update({'biorep_n': biorep_n})

    # remove any bams that are older than another bam
    # resulting in only the most recent surviving
    for bam in [f for f in original_files
                if f.get('file_format') == 'bam' and
                f.get('biorep_n') == biorep_n and
                common.after(bam.get('date_created'), f.get('date_created'))]:
        original_files.remove(bam)

    try:
        rep1_bam = next(f for f in original_files
                        if f.get('file_format') == 'bam' and
                        f.get('biorep_n') == 1)
    except StopIteration:
        logger.error('%s has no rep1 bam.' % (experiment.get('accession')))
        rep1_bam = None
    try:
        rep2_bam = next(f for f in original_files
                        if f.get('file_format') == 'bam' and
                        f.get('biorep_n') == 2)
    except StopIteration:
        logger.error('%s has no rep2 bam.' % (experiment.get('accession')))
        rep2_bam = None
    logger.debug('get_rep_bams returning %s, %s'
                 % (rep1_bam.get('accession'), rep2_bam.get('accession')))
    return rep1_bam, rep2_bam


def get_rep_fastqs(experiment, keypair, server, repn):
    fastq_valid_status = ['released', 'in progress', 'uploaded']
    logger.debug('in get_rep_fastqs with experiment[accession] %s rep %d'
                 % (experiment.get('accession'), repn))

    original_files = \
        [common.encoded_get(urlparse.urljoin(server, '%s' % (uri)), keypair)
         for uri in experiment.get('original_files')]

    fastqs = \
        [f for f in original_files
         if f.get('file_format') == 'fastq' and
         f.get('status') in fastq_valid_status]

    # resolve the biorep_n for each fastq
    rep_fastqs = \
        [f for f in fastqs
         if common.encoded_get(
            urlparse.urljoin(
                server, '%s'
                % (f.get('replicate'))), keypair).get(
                'biological_replicate_number') == repn]
    logger.debug('get_rep_fastqs returning %s'
                 % ([f.get('accession') for f in rep_fastqs]))
    return rep_fastqs


def get_stage_metadata(analysis, stage_name):
    logger.debug('in get_stage_metadata with analysis %s and stage_name %s'
                 % (analysis['id'], stage_name))
    return next(s['execution']
                for s in analysis.get('stages')
                if re.match(stage_name, s['execution']['name']))


def get_experiment_accession(analysis):

    m_executableName = \
        re.search('(ENCSR[0-9]{3}[A-Z]{3})', analysis['executableName'])

    m_name = \
        re.search('(ENCSR[0-9]{3}[A-Z]{3})', analysis['name'])

    if not (m_executableName or m_name):
        logger.error("No experiment accession in name %s or executableName %s."
                     % (analysis['name'], analysis['executableName']))
        return
    elif (m_executableName and m_name):
        executableName_accession = m_executableName.group(1)
        name_accession = m_name.group(1)
        if executableName_accession == name_accession:
            return executableName_accession
        else:
            logger.error(
                'Different experiment accessions: name %s, executableName %s.'
                % (analysis['name'], analysis['executableName']))
            return None
    else:
        m = (m_executableName or m_name)
        experiment_accession = m.group(1)
        logger.debug("get_experiment_accession returning %s"
                     % (experiment_accession))
        return experiment_accession


def get_encoded_repn(mapping_analysis):
    # this is a fragile way to infer the rep number.  It depends on the name of
    # the mapping analysis.  But since there is nowhere in the anlysis input
    # to put it, the only alternative is to infer it from the list of fastq
    # accessions in the inputs.
    # Probably the best thing is to add that as an input into the analysis,
    # then it would be recorded in the analysis metadata
    logger.debug("in get_encoded_repn with analysis[name] %s"
                 % (mapping_analysis['name']))

    m_name = re.search(
        'Map ENCSR[0-9]{3}[A-Z]{3} rep(\d+)', mapping_analysis['name'])

    if not m_name:
        logger.error("Could not infer ENCODED repn from analysis name: %s"
                     % (mapping_analysis['name']))
        return
    else:
        logger.debug("in get_encoded_repn and found repn to be %s"
                     % (m_name.group(1)))
        encoded_repn = int(m_name.group(1))
        return encoded_repn


def get_raw_mapping_stages(mapping_analysis, keypair, server, repn):
    logger.debug(
        'in get_raw_mapping_stages with mapping analysis %s and rep %s'
        % (mapping_analysis['id'], repn))

    experiment_accession = get_experiment_accession(mapping_analysis)

    experiment = common.encoded_get(
        urlparse.urljoin(server, '/experiments/%s'
                                 % (experiment_accession)), keypair)

    # This encoded_repn is the biological_replicate_number at ENCODEd,
    # which needs to be puzzled out from the mapping analysis name
    # or, better, by inferring the rep number from the fastqs actually
    # imported into the analysis
    encoded_repn = get_encoded_repn(mapping_analysis)

    experiment_fastqs = \
        get_rep_fastqs(experiment, keypair, server, encoded_repn)

    experiment_fastq_accessions = \
        [f.get('accession') for f in experiment_fastqs]

    logger.info('%s: Found accessioned experiment fastqs with accessions %s'
                % (experiment_accession, experiment_fastq_accessions))

    mapping_stages = mapping_analysis.get('stages')

    input_stage = \
        next(stage for stage in mapping_stages
             if stage['execution']['name'].startswith("Gather inputs"))

    input_fastq_accessions = input_stage['execution']['input']['reads1']

    if input_stage['execution']['input']['reads2']:
        input_fastq_accessions.append(
            input_stage['execution']['input']['reads2'])

    fastqs = []
    for acc in input_fastq_accessions:
        fobj = common.encoded_get(
            urlparse.urljoin(server, 'files/%s' % (acc)), keypair)
        # logger.debug('fobj')
        # logger.debug('%s' %(pprint.pprint(fobj)))
        fastqs.append(fobj)

    logger.info('Found input fastq objects with accessions %s'
                % ([f.get('accession') for f in fastqs]))

    # Error if it appears we're trying to accession an out-dated analysis
    # (i.e. one not derived from proper fastqs ... maybe some added or revoked)
    if cmp(sorted(flat(experiment_fastq_accessions)),
           sorted(flat(input_fastq_accessions))):
        logger.error(
            '%s rep%d: Accessioned experiment fastqs differ from analysis.'
            % (experiment_accession, repn) +
            'Experiment probably needs remapping')
        return None

    raw_mapping_stage = next(
        stage for stage in mapping_stages
        if stage['execution']['name'].startswith("Map ENCSR"))

    bam = dxpy.describe(
        raw_mapping_stage['execution']['output']['mapped_reads'])

    # here we get the actual DNAnexus file that was used as the reference
    reference_file = dxpy.describe(
        input_stage['execution']['output']['output_JSON']['reference_tar'])

    # and construct the alias to find the corresponding file at ENCODEd
    reference_alias = "dnanexus:" + reference_file.get('id')

    logger.debug('looking for reference file with alias %s'
                 % (reference_alias))

    reference = common.encoded_get(
        urlparse.urljoin(server, 'files/%s' % (reference_alias)), keypair)

    if reference:
        logger.debug('found reference file %s' % (reference.get('accession')))
    else:
        logger.error('failed to find reference file %s' % (reference_alias))

    bam_metadata = common.merge_dicts({
        'file_format': 'bam',
        'output_type': 'alignments'
        }, common_metadata)

    rep_mapping_stages = {
        "Map ENCSR*": {
            'input_files': [

                {'name': 'rep%s_fastqs' % (repn),
                 'derived_from': None,
                 'metadata': None,
                 'encode_object': fastqs},

                {'name': 'reference',
                 'derived_from': None,
                 'metadata': None,
                 'encode_object': reference}

            ],

            'output_files': [

                {'name': 'mapped_reads',
                 'derived_from': ['rep%s_fastqs' % (repn), 'reference'],
                 'metadata': bam_metadata}

            ],

            'qc': [qc],

            'stage_metadata': {}  # initialized below
        }
    }

    for stage_name in rep_mapping_stages:
        if not stage_name.startswith('_'):
            rep_mapping_stages[stage_name].update(
                {'stage_metadata': get_stage_metadata(
                    mapping_analysis, stage_name)})

    return rep_mapping_stages


def get_mapping_stages(mapping_analysis, keypair, server, repn):
    logger.debug('in get_mapping_stages with mapping analysis %s and rep %s'
                 % (mapping_analysis['id'], repn))

    experiment_accession = get_experiment_accession(mapping_analysis)

    experiment = common.encoded_get(
        urlparse.urljoin(
            server, '/experiments/%s' % (experiment_accession)), keypair)

    # This encoded_repn is the biological_replicate_number at ENCODEd, which
    # needs to be puzzled out from the mapping analysis name or, better, by
    # inferring the rep number from the fastqs actually imported into the
    # analysis
    encoded_repn = get_encoded_repn(mapping_analysis)

    experiment_fastqs = \
        get_rep_fastqs(experiment, keypair, server, encoded_repn)

    experiment_fastq_accessions = \
        [f.get('accession') for f in experiment_fastqs]

    logger.info('%s: Found accessioned experiment fastqs with accessions %s'
                % (experiment_accession, experiment_fastq_accessions))

    mapping_stages = mapping_analysis.get('stages')

    input_stage = next(
        stage for stage in mapping_stages
        if stage['execution']['name'].startswith("Gather inputs"))

    input_fastq_accessions = input_stage['execution']['input']['reads1']

    if input_stage['execution']['input']['reads2']:
        input_fastq_accessions.append(
            input_stage['execution']['input']['reads2'])

    fastqs = []

    for acc in input_fastq_accessions:
        fobj = common.encoded_get(
            urlparse.urljoin(server, 'files/%s' % (acc)), keypair)
        # logger.debug('fobj')
        # logger.debug('%s' %(pprint.pprint(fobj)))
        fastqs.append(fobj)

    logger.info('Found input fastq objects with accessions %s'
                % ([f.get('accession') for f in fastqs]))

    # Error if it appears we're trying to accession an out-dated analysis
    # (i.e. one not derived from proper fastqs ... maybe some added or revoked)
    if cmp(sorted(flat(experiment_fastq_accessions)),
           sorted(flat(input_fastq_accessions))):
        logger.error(
            '%s rep%d: Accessioned experiment fastqs differ from analysis.'
            % (experiment_accession, repn) +
            'Experiment probably needs remapping')
        return None

    filter_qc_stage = next(
        stage for stage in mapping_stages
        if stage['execution']['name'].startswith("Filter and QC"))

    bam = dxpy.describe(filter_qc_stage['execution']['output']['filtered_bam'])

    # here we get the actual DNAnexus file that was used as the reference
    reference_file = dxpy.describe(
        input_stage['execution']['output']['output_JSON']['reference_tar'])

    # and construct the alias to find the corresponding file at ENCODEd
    reference_alias = "dnanexus:" + reference_file.get('id')

    logger.debug('looking for reference file with alias %s'
                 % (reference_alias))

    reference = common.encoded_get(
        urlparse.urljoin(server, 'files/%s' % (reference_alias)), keypair)

    if reference:
        logger.debug('found reference file %s' % (reference.get('accession')))
    else:
        logger.error('failed to find reference file %s' % (reference_alias))

    bam_metadata = common.merge_dicts({
        'file_format': 'bam',
        'output_type': 'alignments'
        }, common_metadata)

    rep_mapping_stages = {

        "Map ENCSR*": {
            'input_files': [],
            'output_files': [],
            'qc': [],
            'stage_metadata': {}  # initialized below
        },

        "Filter and QC*": {
            'input_files': [

                {'name': 'rep%s_fastqs' % (repn),
                 'derived_from': None,
                 'metadata': None,
                 'encode_object': fastqs},

                {'name': 'reference',
                 'derived_from': None,
                 'metadata': None,
                 'encode_object': reference}

            ],

            'output_files': [
                {'name': 'filtered_bam',
                 'derived_from': ['rep%s_fastqs' % (repn), 'reference'],
                 'metadata': bam_metadata}
            ],

            'qc': [qc, dup_qc, pbc_qc, filtered_qc, xcor_qc],

            'stage_metadata': {}  # initialized below
        },

        "Calculate cross-correlation*": {
            'input_files': [],
            'output_files': [],
            'qc': [],
            'stage_metadata': {}
        }
    }

    for stage_name in rep_mapping_stages:
        if not stage_name.startswith('_'):
            rep_mapping_stages[stage_name].update(
                {'stage_metadata': get_stage_metadata(
                    mapping_analysis, stage_name)})

    return rep_mapping_stages


def get_control_mapping_stages(peaks_analysis, experiment, keypair, server,
                               reps=[1, 2]):
    # Find the control inputs
    logger.debug(
        'in get_control_mapping_stages with peaks_analysis %s'
        % (peaks_analysis['id']) +
        'experiment %s; reps %s' % (experiment['accession'], reps))

    peaks_stages = peaks_analysis.get('stages')

    peaks_stage = next(
        stage for stage in peaks_stages
        if stage['execution']['name'] == "ENCODE Peaks")

    # here reps is always 1,2 because the peaks job rep numbers are 1,2 ...
    # these are not the ENCODEd biological_replicate_numbers, which are
    # only known to the analysis via its name, or by going back to ENCODEd and
    # figuring out where the fastqs came from
    tas = [dxpy.describe(peaks_stage['execution']['input']['ctl%s_ta' % (n)])
           for n in reps]

    mapping_jobs = [dxpy.describe(ta['createdBy']['job']) for ta in tas]

    mapping_analyses = [dxpy.describe(mapping_job['analysis'])
                        for mapping_job in mapping_jobs]

    mapping_stages = []

    for (i, repn) in enumerate(reps):
        mapping_stage = get_mapping_stages(
            mapping_analyses[i], keypair, server, repn)

        if not mapping_stage:
            logger.error('%s: failed to find mapping stages for rep%d'
                         % (peaks_analysis['id'], repn))
            return None
        else:
            mapping_stages.append(mapping_stage)

    return mapping_stages


def get_peak_mapping_stages(peaks_analysis, experiment, keypair, server,
                            reps=[1, 2]):

    # Find the tagaligns actually used as inputs into the analysis
    # Find the mapping analyses that produced those tagaligns
    # Find the filtered bams from those analyses
    # Build the stage dict and return it

    if not peaks_analysis:
        logger.debug(
            'in get_peak_mapping_stages: peaks_analysis is %s'
            % (peaks_analysis))
    elif not experiment:
        logger.debug(
            'in get_peak_mapping_stages: experiment is %s'
            % (experiment))
    elif not reps:
        logger.debug(
            'in get_peak_maping_stages: reps is %s'
            % (reps))
    else:
        logger.debug('in get_peak_mapping_stages with peaks_analysis %s'
                     % (peaks_analysis['id']) +
                     'experiment %s; reps %s'
                     % (experiment['accession'], reps))

    peaks_stages = peaks_analysis.get('stages')

    peaks_stage = next(
        stage for stage in peaks_stages
        if stage['execution']['name'] == "ENCODE Peaks")

    tas = [dxpy.describe(peaks_stage['execution']['input']['rep%s_ta' % (n)])
           for n in reps]

    mapping_jobs = \
        [dxpy.describe(ta['createdBy']['job'])
         for ta in tas]

    mapping_analyses = \
        [dxpy.describe(mapping_job['analysis'])
         for mapping_job in mapping_jobs]

    mapping_stages = []
    for (i, repn) in enumerate(reps):
        mapping_stage = \
            get_mapping_stages(mapping_analyses[i], keypair, server, repn)
        if not mapping_stage:
            logger.error('%s: failed to find mapping stages for rep%d'
                         % (peaks_analysis['id'], repn))
            return None
        else:
            mapping_stages.append(mapping_stage)

    return mapping_stages


def pooled_controls(peaks_analysis, rep):
    # this is not surfaced explicitly so must be inferred
    # General:  get the id's of the files actually used for the specified rep
    # and pooled controls.  If the id is the same as the pooled control id then
    # return true.
    # Specifically:
    # starting with the peaks_analysis, get its stages
    # get "ENCODE Peaks" stage
    # get the job id for "ENCODE Peaks"
    # get the control and experiment file ID's for the specified rep
    # find the child jobs of the "ENCODE Peaks" job
    # find the child job where macs2 was run with the experiment file
    # corresponding to the experiment file for this rep
    # get from that child job the file ID of the control
    # if the contol file ID for this rep from ENCODE Peaks is the same as in
    # macs2 then return False else return True
    # Could double-check the log output of ENCODE Peaks to search for the
    # strings "Using pooled controls for replicate 1.", "Using pooled controls
    # for replicate 2." and "Using pooled controls."
    # But there is no corresponding "Not pooling controls"
    # message, so it's underdertermined.

    logger.debug('in pooled_controls with peaks_analysis %s; rep %s'
                 % (peaks_analysis['id'], rep))

    peaks_stages = peaks_analysis.get('stages')

    ENCODE_Peaks_stage = next(
        stage for stage in peaks_stages
        if stage['execution']['name'] == "ENCODE Peaks")

    ENCODE_Peaks_exp_file = \
        ENCODE_Peaks_stage['execution']['input']['rep%s_ta' % (rep)]

    ENCODE_Peaks_ctl_file = \
        ENCODE_Peaks_stage['execution']['input']['ctl%s_ta' % (rep)]

    # print ENCODE_Peaks_stage['execution']['id']
    # print ENCODE_Peaks_stage['execution']['project']

    child_jobs = dxpy.find_jobs(
        parent_job=ENCODE_Peaks_stage['execution']['id'],
        name="MACS2",
        project=ENCODE_Peaks_stage['execution']['project'],
        describe=True)

    rep_job = next(
        job for job in child_jobs
        if job['describe']['input']['experiment'] == ENCODE_Peaks_exp_file)

    # for job in child_jobs:
    #   #pprint.pprint(job)
    #   if job['describe']['input']['experiment'] == ENCODE_Peaks_exp_file:
    #       rep_job = job

    rep_job_ctl_file = rep_job['describe']['input']['control']

    logger.info("Rep%s input control file %s; actually used %s"
                % (rep, ENCODE_Peaks_ctl_file, rep_job_ctl_file))

    if ENCODE_Peaks_ctl_file == rep_job_ctl_file:
        logger.info('Inferred controls not pooled for rep%s' % (rep))
        return False
    else:
        logger.info('Inferred pooled controls for rep%s' % (rep))
        return True


def get_histone_peak_stages(peaks_analysis, mapping_stages, control_stages,
                            experiment, keypair, server):

    logger.debug(
        'in get_histone_peak_stages with peaks_analysis %s;'
        % (peaks_analysis['id']) +
        'experiment %s and len(mapping_stages) %d len(control_stages) %d'
        % (experiment['accession'], len(mapping_stages), len(control_stages)))

    narrowpeak_metadata = common.merge_dicts(
        {'file_format': 'bed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'peaks'},
        common_metadata)

    replicated_narrowpeak_metadata = common.merge_dicts(
        {'file_format': 'bed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'replicated peaks'},
        common_metadata)

    narrowpeak_bb_metadata = common.merge_dicts({
        'file_format': 'bigBed',
        'file_format_type': 'narrowPeak',
        'file_format_specifications': ['ENCODE:narrowPeak.as'],
        'output_type': 'peaks'},
        common_metadata)

    replicated_narrowpeak_bb_metadata = common.merge_dicts({
        'file_format': 'bigBed',
        'file_format_type': 'narrowPeak',
        'file_format_specifications': ['ENCODE:narrowPeak.as'],
        'output_type': 'replicated peaks'},
        common_metadata)

    fc_signal_metadata = common.merge_dicts({
        'file_format': 'bigWig',
        'output_type': 'fold change over control'},
        common_metadata)

    pvalue_signal_metadata = common.merge_dicts({
        'file_format': 'bigWig',
        'output_type': 'signal p-value'},
        common_metadata)

    rep1_bam, rep2_bam = \
        [(mapping_stages[n], 'filtered_bam') for n in range(2)]

    rep1_ctl_bam, rep2_ctl_bam = \
        [(control_stages[n], 'filtered_bam') for n in range(2)]

    pooled_ctl_bams = [rep1_ctl_bam, rep2_ctl_bam]

    if pooled_controls(peaks_analysis, rep=1):
        rep1_ctl = pooled_ctl_bams
    else:
        rep1_ctl = [rep1_ctl_bam]
    if pooled_controls(peaks_analysis, rep=2):
        rep2_ctl = pooled_ctl_bams
    else:
        rep2_ctl = [rep2_ctl_bam]

    peak_stages = {
        # derived_from is by name here, will be patched into the file metadata
        # after all files are accessioned
        # derived_from can also be a tuple of (stages,name) to connect to#
        # files outside of this set of stages
        "ENCODE Peaks": {
            'output_files': [

                {'name': 'rep1_narrowpeaks',
                 'derived_from': [rep1_bam] + rep1_ctl,
                 'metadata': narrowpeak_metadata},

                {'name': 'rep2_narrowpeaks',
                 'derived_from': [rep2_bam] + rep2_ctl,
                 'metadata': narrowpeak_metadata},

                {'name': 'pooled_narrowpeaks',
                 'derived_from': [rep1_bam, rep2_bam] + pooled_ctl_bams,
                 'metadata': narrowpeak_metadata},

                {'name': 'rep1_narrowpeaks_bb',
                 'derived_from': ['rep1_narrowpeaks'],
                 'metadata': narrowpeak_bb_metadata},

                {'name': 'rep2_narrowpeaks_bb',
                 'derived_from': ['rep2_narrowpeaks'],
                 'metadata': narrowpeak_bb_metadata},

                {'name': 'pooled_narrowpeaks_bb',
                 'derived_from': ['pooled_narrowpeaks'],
                 'metadata': narrowpeak_bb_metadata},

                {'name': 'rep1_pvalue_signal',
                 'derived_from': [rep1_bam] + rep1_ctl,
                 'metadata': pvalue_signal_metadata},

                {'name': 'rep2_pvalue_signal',
                 'derived_from': [rep2_bam] + rep2_ctl,
                 'metadata': pvalue_signal_metadata},

                {'name': 'pooled_pvalue_signal',
                 'derived_from': [rep1_bam, rep2_bam] + pooled_ctl_bams,
                 'metadata': pvalue_signal_metadata},

                {'name': 'rep1_fc_signal',
                 'derived_from': [rep1_bam] + rep1_ctl,
                 'metadata': fc_signal_metadata},

                {'name': 'rep2_fc_signal',
                 'derived_from': [rep2_bam] + rep2_ctl,
                 'metadata': fc_signal_metadata},

                {'name': 'pooled_fc_signal',
                 'derived_from': [rep1_bam, rep2_bam] + pooled_ctl_bams,
                 'metadata': fc_signal_metadata}
            ],

            'qc': [],

            'stage_metadata': {}  # initialized below
        },

        "Overlap narrowpeaks": {
            'output_files': [

                {'name': 'overlapping_peaks',
                 'derived_from': ['rep1_narrowpeaks', 'rep2_narrowpeaks',
                                  'pooled_narrowpeaks'],
                 'metadata': replicated_narrowpeak_metadata},

                {'name': 'overlapping_peaks_bb',
                 'derived_from': ['overlapping_peaks'],
                 'metadata': replicated_narrowpeak_bb_metadata}
            ],

            'qc': ['npeaks_in', 'npeaks_out', 'npeaks_rejected'],

            'stage_metadata': {}  # initialized below
        }
    }

    for stage_name in peak_stages:
        if not stage_name.startswith('_'):
            peak_stages[stage_name].update(
                {'stage_metadata': get_stage_metadata(
                    peaks_analysis, stage_name)})

    return peak_stages


def get_tf_peak_stages(peaks_analysis, mapping_stages, control_stages,
                       experiment, keypair, server):

    logger.debug(
        'in get_tf_peak_stages with peaks_analysis %s;'
        % (peaks_analysis['id']) +
        'experiment %s and len(mapping_stages) %d len(control_stages) %d'
        % (experiment['accession'], len(mapping_stages), len(control_stages)))

    narrowpeak_metadata = common.merge_dicts(
        {'file_format': 'bed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'peaks'},
        common_metadata)

    idr_optimal_narrowpeak_metadata = common.merge_dicts(
        {'file_format': 'bed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'optimal idr thresholded peaks'},
        common_metadata)

    idr_conservative_narrowpeak_metadata = common.merge_dicts(
        {'file_format': 'bed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'conservative idr thresholded peaks'},
        common_metadata)

    narrowpeak_bb_metadata = common.merge_dicts(
        {'file_format': 'bigBed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'peaks'},
        common_metadata)

    idr_optimal_narrowpeak_bb_metadata = common.merge_dicts(
        {'file_format': 'bigBed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'optimal idr thresholded peaks'},
        common_metadata)

    idr_conservative_narrowpeak_bb_metadata = common.merge_dicts(
        {'file_format': 'bigBed',
         'file_format_type': 'narrowPeak',
         'file_format_specifications': ['ENCODE:narrowPeak.as'],
         'output_type': 'conservative idr thresholded peaks'},
        common_metadata)

    fc_signal_metadata = common.merge_dicts(
        {'file_format': 'bigWig',
         'output_type': 'fold change over control'},
        common_metadata)

    pvalue_signal_metadata = common.merge_dicts(
        {'file_format': 'bigWig',
         'output_type': 'signal p-value'},
        common_metadata)

    rep1_bam, rep2_bam = \
        [(mapping_stages[n], 'filtered_bam') for n in range(2)]

    rep1_ctl_bam, rep2_ctl_bam = \
        [(control_stages[n], 'filtered_bam') for n in range(2)]

    pooled_ctl_bams = [rep1_ctl_bam, rep2_ctl_bam]

    if pooled_controls(peaks_analysis, rep=1):
        rep1_ctl = pooled_ctl_bams
    else:
        rep1_ctl = [rep1_ctl_bam]

    if pooled_controls(peaks_analysis, rep=2):
        rep2_ctl = pooled_ctl_bams
    else:
        rep2_ctl = [rep2_ctl_bam]

    peak_stages = {
        # derived_from is by name here, will be patched into the file metadata
        # after all files are accessioned
        # derived_from can also be a tuple of (stages,name) to connect to files
        # outside of this set of stages

        "SPP Peaks": {
            'output_files': [

                {'name': 'rep1_peaks',
                 'derived_from': [rep1_bam] + rep1_ctl,
                 'metadata': narrowpeak_metadata},

                {'name': 'rep2_peaks',
                 'derived_from': [rep2_bam] + rep2_ctl,
                 'metadata': narrowpeak_metadata},

                {'name': 'pooled_peaks',
                 'derived_from': [rep1_bam, rep2_bam] + pooled_ctl_bams,
                 'metadata': narrowpeak_metadata},

                {'name': 'rep1_peaks_bb',
                 'derived_from': ['rep1_peaks'],
                 'metadata': narrowpeak_bb_metadata},

                {'name': 'rep2_peaks_bb',
                 'derived_from': ['rep2_peaks'],
                 'metadata': narrowpeak_bb_metadata},

                {'name': 'pooled_peaks_bb',
                 'derived_from': ['pooled_peaks'],
                 'metadata': narrowpeak_bb_metadata}
            ],

            'qc': [],

            'stage_metadata': {}  # initialized below
        },

        "ENCODE Peaks": {

            'output_files': [

                {'name': 'rep1_pvalue_signal',
                 'derived_from': [rep1_bam] + rep1_ctl,
                 'metadata': pvalue_signal_metadata},

                {'name': 'rep2_pvalue_signal',
                 'derived_from': [rep2_bam] + rep2_ctl,
                 'metadata': pvalue_signal_metadata},

                {'name': 'pooled_pvalue_signal',
                 'derived_from': [rep1_bam, rep2_bam] + pooled_ctl_bams,
                 'metadata': pvalue_signal_metadata},

                {'name': 'rep1_fc_signal',
                 'derived_from': [rep1_bam] + rep1_ctl,
                 'metadata': fc_signal_metadata},

                {'name': 'rep2_fc_signal',
                 'derived_from': [rep2_bam] + rep2_ctl,
                 'metadata': fc_signal_metadata},

                {'name': 'pooled_fc_signal',
                 'derived_from': [rep1_bam, rep2_bam] + pooled_ctl_bams,
                 'metadata': fc_signal_metadata}
            ],

            'qc': [],

            'stage_metadata': {}  # initialized below
        },

        "IDR True Replicates": {
            'output_files': [],
            'qc': [],
            'stage_metadata': {}  # initialized below
        },

        "IDR Rep 1 Self-pseudoreplicates": {
            'output_files': [],
            'qc': [],
            'stage_metadata': {}  # initialized below
        },

        "IDR Rep 2 Self-pseudoreplicates": {
            'output_files': [],
            'qc': [],
            'stage_metadata': {}  # initialized below
        },

        "IDR Pooled Pseudoreplicates": {
            'output_files': [],
            'qc': [],
            'stage_metadata': {}  # initialized below
        },

        "Final IDR peak calls": {

            'output_files': [

                {'name': 'conservative_set',
                 'derived_from': ['rep1_peaks', 'rep2_peaks', 'pooled_peaks'],
                 'metadata': idr_conservative_narrowpeak_metadata},

                {'name': 'conservative_set_bb',
                 'derived_from': ['conservative_set'],
                 'metadata': idr_conservative_narrowpeak_bb_metadata},

                {'name': 'optimal_set',
                 'derived_from': ['rep1_peaks', 'rep2_peaks', 'pooled_peaks'],
                 'metadata': idr_optimal_narrowpeak_metadata},

                {'name': 'optimal_set_bb',
                 'derived_from': ['optimal_set'],
                 'metadata': idr_optimal_narrowpeak_bb_metadata}

            ],

            'qc': ['reproducibility_test', 'rescue_ratio', 'Np', 'N1', 'N2',
                   'Nt', 'self_consistency_ratio'],

            'stage_metadata': {}  # initialized below
        }
    }

    for stage_name in peak_stages:
        if not stage_name.startswith('_'):
            peak_stages[stage_name].update(
                {'stage_metadata': get_stage_metadata(
                    peaks_analysis, stage_name)})

    return peak_stages


def resolve_name_to_accessions(stages, stage_file_name):
    # given a dict of named stages, and the name of one of the stages' outputs,
    # return that output's ENCODE accession number
    logger.debug("in resolve_name_to_accessions with stage_file_name")
    logger.debug("%s" % (pprint.pformat(stage_file_name)))
    accessions = []
    for stage_name in stages:
        if stages[stage_name].get('input_files'):
            all_files = stages[stage_name].get('output_files') + stages[stage_name].get('input_files')
        else:
            all_files = stages[stage_name].get('output_files')
        for stage_file in all_files:
            if stage_file['name'] == stage_file_name:
                encode_object = stage_file.get('encode_object')
                if isinstance(encode_object,list):
                    for obj in encode_object:
                        accessions.append(obj.get('accession'))
                else:
                    accessions.append(encode_object.get('accession'))
    if accessions:
        logger.debug('resolve_name_to_accessons returning:')
        logger.debug('%s' %(pprint.pformat(accessions)))
        return accessions
    else:
        logger.warning('Failed to resolve to accessions, stage_file_name:')
        logger.warning('%s' %(pprint.pformat(stage_file_name)))
        return None

def patch_file(payload, keypair, server, dryrun):
    logger.debug('in patch_file with %s' %(pprint.pformat(payload)))
    accession = payload.pop('accession')
    url = urlparse.urljoin(server,'files/%s' %(accession))
    if dryrun:
        logger.info("Dry run.  Would PATCH: %s with %s" %(accession, pprint.pformat(payload)))
        logger.info("Dry run.  Returning unchanged file object")
        new_file_object = common.encoded_get(urlparse.urljoin(server,'/files/%s' %(accession)), keypair)
    else:
        # r = requests.patch(url, auth=keypair, headers={'content-type': 'application/json'}, data=json.dumps(payload))
        r = common.encoded_patch(url, keypair, payload, return_response=True)
        try:
            r.raise_for_status()
        except:
            logger.error('PATCH file object failed: %s %s' % (r.status_code, r.reason))
            logger.error(r.text)
            new_file_object = None
        else:
            new_file_object = r.json()['@graph'][0]
            logger.info("Patched: %s" %(new_file_object.get('accession')))
    
    return new_file_object

def post_file(payload, keypair, server, dryrun):
    logger.debug('in post_file with %s' %(pprint.pformat(payload)))
    url = urlparse.urljoin(server,'files/')
    if dryrun:
        logger.info("Dry run.  Would post: %s" %(pprint.pformat(payload)))
        new_file_object = None
    else:
        # r = requests.post(url, auth=keypair, headers={'content-type': 'application/json'}, data=json.dumps(payload))
        r = common.encoded_post(url, keypair, payload, return_response=True)
        try:
            r.raise_for_status()
        except:
            logger.error('POST file object failed: %s %s' % (r.status_code, r.reason))
            logger.error(r.text)
            new_file_object = None
        else:
            new_file_object = r.json()['@graph'][0]
            logger.info("New accession: %s" %(new_file_object.get('accession')))
    
    return new_file_object

def accession_file(f, keypair, server, dryrun, force):
    #check for duplication
    #- if it has ENCFF or TSTFF number in it's tag, or
    #- if there exists an accessioned file with the same submitted_file_name that is not deleted, replaced, revoked and has the same size
    #- then there should be a file with the same md5.  If not, warn of a mismatch between what's at DNAnexus and ENCODEd.
    #- If same md5, return the existing object.  
    #- Next, check if there's already a file with the same md5.  If it's deleted, replaced, revoked, then remodel it if --force=true,
    #- Else warn and return None
    #download
    #calculate md5 and add to f.md5sum
    #post file and get accession, upload credentials
    #upload to S3
    #remove the local file (to save space)
    #return the ENCODEd file object
    logger.debug('in accession_file with f %s' %(pprint.pformat(f['submitted_file_name'])))
    dx = f.pop('dx')

    local_fname = dx.name
    logger.info("Downloading %s" %(local_fname))
    dxpy.download_dxfile(dx.get_id(),local_fname)
    f.update({'md5sum': common.md5(local_fname)})
    f['notes'] = json.dumps(f.get('notes'))

    #check to see if md5 already in the database
    url = server + '/md5:%s?format=json&frame=object' %(f.get('md5sum'))
    r = common.encoded_get(url, keypair, return_response=True)
    try:
        r.raise_for_status()
    except:
        if r.status_code == 404:
            logger.info('No md5 matches %s' %(f.get('md5sum')))
            md5_exists = False
        else:
            logger.error('MD5 duplicate check. GET failed: %s %s' % (r.status_code, r.reason))
            logger.error(r.text)
            md5_exists = None
    else:
        md5_exists = r.json()

    #check if an ENCODE accession number in in the list of tags, as it would be if accessioned by this script or similar scripts
    for tag in dx.tags:
        m = re.findall(r'ENCFF\d{3}\D{3}', tag)
        if m:
            logger.info('%s appears to contain ENCODE accession number in tag %s.' %(dx.get_id(),m))
            accession_in_tag = True
            # if not force:
            #   return
        else:
            accession_in_tag = False

    #TODO check here if file is deprecated and, if so, warn
    if md5_exists:
        if force:
            return patch_file(f, keypair, server, dryrun)
        else:
            logger.info("Returning duplicate file unchanged")
            return md5_exists
    else:
        logger.info('posting new file %s' %(f.get('submitted_file_name')))
        logger.debug('%s' %(f))
        new_file_object = post_file(f, keypair, server, dryrun)


    if new_file_object:
        creds = new_file_object['upload_credentials']
        env = os.environ.copy()
        env.update({
            'AWS_ACCESS_KEY_ID': creds['access_key'],
            'AWS_SECRET_ACCESS_KEY': creds['secret_key'],
            'AWS_SECURITY_TOKEN': creds['session_token'],
        })

        logger.info("Uploading file.")
        start = time.time()
        try:
            subprocess.check_call(['aws', 's3', 'cp', local_fname, creds['upload_url'], '--quiet'], env=env)
        except subprocess.CalledProcessError as e:
            # The aws command returns a non-zero exit code on error.
            logger.error("Upload failed with exit code %d" % e.returncode)
        else:
            end = time.time()
            duration = end - start
            logger.info("Uploaded in %.2f seconds" % duration)
            dx.add_tags([new_file_object.get('accession')])

    try:
        os.remove(local_fname)
    except:
        pass

    return new_file_object

def accession_analysis_step_run(analysis_step_run_metadata, keypair, server, dryrun, force):
    url = urlparse.urljoin(server,'/analysis-step-runs/')
    if dryrun:
        logger.info("Dry run.  Would POST %s" %(analysis_step_run_metadata))
        new_object = {}
    else:
        # r = requests.post(url, auth=keypair, headers={'content-type': 'application/json'}, data=json.dumps(analysis_step_run_metadata))
        r = common.encoded_post(url, keypair, analysis_step_run_metadata, return_response=True)
        try:
            r.raise_for_status()
        except:
            if r.status_code == 409:
                url = urlparse.urljoin(server,"/%s" %(analysis_step_run_metadata['aliases'][0])) #assumes there's only one alias
                new_object = common.encoded_get(url, keypair)
                logger.info('Using existing analysis_step_run object %s' %(new_object.get('@id')))
            else:
                logger.warning('POST analysis_step_run object failed: %s %s' % (r.status_code, r.reason))
                logger.warning(r.text)
                new_object = {}
        else:
            new_object = r.json()['@graph'][0]
            logger.info("New analysis_step_run uuid: %s" %(new_object.get('uuid')))
    return new_object

def accession_outputs(stages, experiment, keypair, server, dryrun, force):
    files = []
    for (stage_name, outputs) in stages.iteritems():
        stage_metadata = outputs['stage_metadata']
        for i,file_metadata in enumerate(outputs['output_files']):
            project = stage_metadata['project']
            dx = dxpy.DXFile(stage_metadata['output'][file_metadata['name']], project=project)
            dx_desc = dx.describe()
            surfaced_outputs = [o for o in outputs['qc'] if isinstance(o,str)] #this will be a list of strings
            calculated_outputs = [o for o in outputs['qc'] if not isinstance(o,str)] #this will be a list of functions/methods
            notes_qc = dict(zip(surfaced_outputs,[stage_metadata['output'][metric] for metric in surfaced_outputs]))
            notes_qc.update(dict(zip([f.__name__ for f in calculated_outputs],[f(stages) for f in calculated_outputs])))
            post_metadata = {
                'dx': dx,
                'notes': {
                    'dx-id': dx.get_id(),
                    'dx-createdBy': dx_desc.get('createdBy'),
                    'qc': notes_qc
                },
                #'aliases': ['ENCODE:%s-%s' %(experiment.get('accession'), static_metadata.pop('name'))],
                'dataset': experiment.get('accession'),
                'file_size': dx_desc.get('size'),
                'submitted_file_name': dx.get_proj_id() + ':' + '/'.join([dx.folder,dx.name])}
            post_metadata.update(file_metadata['metadata'])
            new_file = accession_file(post_metadata, keypair, server, dryrun, force)
            stages[stage_name]['output_files'][i].update({'encode_object': new_file})
            files.append(new_file)
    return files

def patch_outputs(stages, keypair, server, dryrun):
    logger.debug('in patch_outputs')
    files = []
    for stage_name in stages:
        for n,file_metadata in enumerate(stages[stage_name]['output_files']):
            if file_metadata.get('encode_object'):
                logger.info('patch outputs stage_name %s n %s file_metadata[name] %s encode_object[accession] %s' %(stage_name, n, file_metadata['name'], file_metadata['encode_object'].get('accession')))
                logger.debug("encode_object %s" %(pprint.pformat(file_metadata['encode_object']['@id'])))
                logger.debug("patch_outputs file_metadata")
                # logger.debug("%s" %(pprint.pformat(file_metadata)))
                accession = file_metadata['encode_object'].get('accession')
                derived_from_accessions = set() #no duplicates allowed
                for derived_from in file_metadata['derived_from']:
                    #derived from can be a tuple specifying a name from different set of stages
                    if isinstance(derived_from,tuple):
                        logger.debug('different stage file_metadata[derived_from] =')
                        logger.debug('%s' %(pprint.pformat(derived_from[1])))
                        stages_to_use = derived_from[0]
                        name_to_use = derived_from[1]
                    else:
                        logger.debug('same stage file_metadata[derived_from]')
                        logger.debug('%s' %(pprint.pformat(derived_from)))
                        stages_to_use = stages
                        name_to_use = derived_from
                    #May see the same accession twice.  If, for example, a single control is reused, there
                    #will be two paths back to it (one via rep1 one via rep2) and so it will come out of this loop twice.
                    for acc in resolve_name_to_accessions(stages_to_use, name_to_use):
                        derived_from_accessions.add(acc)
                logger.debug('derived_from_accessions = %s' %(pprint.pformat(derived_from_accessions)))
                patch_metadata = {
                    'accession': accession,
                    'derived_from': list(derived_from_accessions)
                }
                logger.debug('patch_metadata = %s' %(pprint.pformat(patch_metadata)))
                patched_file = patch_file(patch_metadata, keypair, server, dryrun)
                if patched_file:
                    stages[stage_name]['output_files'][n]['encode_object'] = patched_file
                    files.append(patched_file)
                else:
                    logger.error("%s PATCH failed ... skipping" %(accession))
            else:
                logger.warning('%s,%s: No encode object found ... skipping' %(stage_name, file_metadata['name']))
                continue
    return files

def accession_qc_object(obj_type, obj, keypair, server, dryrun, force):
    logger.debug('in accession_qc_object with obj_type %s obj.keys() %s' %(obj_type, obj.keys()))
    logger.debug('obj[step_run] %s' %(obj.get('step_run')))
    #To avoid duplicating qc objects check for same analysis_step_run, and if there is a qc object of this type
    #for that analysis_step run, then delete it first.
    #Result is that the QC objects from the analysis being accessioned will supercede those that already exist.
    url = urlparse.urljoin(server,'/search/?type=%s&step_run=%s' %(obj_type, obj.get('step_run')))
    logger.debug('url %s' %(url))
    r = common.encoded_get(url,keypair)
    objects = [o for o in r['@graph'] if o['status'] not in DEPRECATED]
    logger.debug('found %d qc objects of type %s' %(len(objects), obj_type))
    existing_objects = [o for o in objects if o.get('step_run') == obj['step_run']]
    if existing_objects:
        existing_object = existing_objects.pop()
    else:
        existing_object = None
    for object_to_delete in existing_objects:
        url = urlparse.urljoin(server,object_to_delete['@id'])
        common.encoded_patch(url, keypair, {'status':'deleted'})

    payload = json.dumps(obj)
    if existing_object:
        url = urlparse.urljoin(server, existing_object['@id'])
        logger.debug('patching %s with %s' %(url,payload))
        # r = requests.patch(url, auth=keypair, headers={'content-type': 'application/json'}, data=payload)
        r = common.encoded_patch(url, keypair, obj, return_response=True)
    else:
        url = urlparse.urljoin(server, '/%s/' %(obj_type))
        logger.debug('posting to %s with %s' %(url,payload))
        # r = requests.post(url, auth=keypair, headers={'content-type': 'application/json'}, data=payload)
        r = common.encoded_post(url, keypair, obj, return_response=True)
    try:
        r.raise_for_status()
    except:
        logger.error('PATCH or POST failed: %s %s' % (r.status_code, r.reason))
        logger.error('url was %s' %(url))
        logger.error(r.text)
        new_qc_object = None
    else:
        new_qc_object = r.json()['@graph'][0]

    return new_qc_object

def accession_pipeline(analysis_step_versions, keypair, server, dryrun, force):
    patched_files = []
    for (analysis_step_version_name, steps) in analysis_step_versions.iteritems():
        for step in steps:
            if not (step['stages'] and step['stage_name'] and step['file_names']):
                logger.warning('%s missing stage metadata (files or stage_name) ... skipping' %(analysis_step_version_name))
                continue
            stage_name = step['stage_name']
            jobid = step['stages'][stage_name]['stage_metadata']['id']
            analysis_step_version = 'versionof:%s' %(analysis_step_version_name)
            alias = 'dnanexus:%s' %(jobid)
            if step.get('status') == 'virtual':
                alias += '-virtual-file-conversion-step'
            analysis_step_run_metadata = {
                'aliases': [alias],
                'analysis_step_version': analysis_step_version,
                'status': step['status'],
                'dx_applet_details': [{
                    'dx_status': 'finished',
                    'dx_job_id': 'dnanexus:%s' %(jobid),
                }]
            }
            analysis_step_run = accession_analysis_step_run(analysis_step_run_metadata, keypair, server, dryrun, force)
            logger.debug('in accession_pipeline analysis_step_run %s' %(pprint.pformat(analysis_step_run)))
            for qc in step['qc_objects']:
                qc_object_name, files_to_associate = next(qc.iteritems())
                qc_objects = globals()[qc_object_name](analysis_step_run.get('@id'), step['stages'], files_to_associate)
                for qc_object in qc_objects:
                    new_object = accession_qc_object(qc_object_name, qc_object, keypair, server, dryrun, force)
                    logger.info('New %s qc object %s aliases %s' %(qc_object_name, new_object.get('uuid'), new_object.get('aliases')))
                    logger.debug('%s' %(pprint.pformat(new_object)))

            for file_name in step['file_names']:
                for file_accession in resolve_name_to_accessions(step['stages'], file_name):
                    patch_metadata = {
                        'accession': file_accession,
                        'step_run': analysis_step_run.get('@id')
                    }
                    patched_file = patch_file(patch_metadata, keypair, server, dryrun)
                    patched_files.append(patched_file)

    return patched_files

def accession_mapping_analysis_files(mapping_analysis, keypair, server, dryrun, force):

    experiment_accession = get_experiment_accession(mapping_analysis)
    if not experiment_accession:
        logger.info("Missing experiment accession or rep in %s, skipping." %(mapping_analysis['name']))
        return []

    m = re.match('^Map (ENCSR[0-9]{3}[A-Z]{3}) rep(\d+)',mapping_analysis['name'])
    if m:
        repn = int(m.group(2))
    else:
        logger.error("Missing rep in %s, skipping." %(mapping_analysis['name']))
        return []

    logger.info("%s rep %d: accessioning mapping." %(experiment_accession, repn))

    experiment = common.encoded_get(urlparse.urljoin(server,'/experiments/%s' %(experiment_accession)), keypair)
    mapping_stages = get_mapping_stages(mapping_analysis, keypair, server, repn)

    output_files = accession_outputs(mapping_stages, experiment, keypair, server, dryrun, force)

    files_with_derived = patch_outputs(mapping_stages, keypair, server, dryrun)

    mapping_analysis_step_versions = {
        'bwa-indexing-step-v-1' : [
            {
                'stages' : "",
                'stage_name': "",
                'file_names' : [],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'bwa-alignment-step-v-1' : [
            {
                'stages' : mapping_stages,
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}
                ]
            }
        ]
    }

    patched_files = accession_pipeline(mapping_analysis_step_versions, keypair, server, dryrun, force)
    return patched_files

def accession_raw_mapping_analysis_files(mapping_analysis, keypair, server, dryrun, force):

    experiment_accession = get_experiment_accession(mapping_analysis)
    if not experiment_accession:
        logger.info("Missing experiment accession or rep in %s, skipping." %(mapping_analysis['name']))
        return []

    m = re.match('^Map (ENCSR[0-9]{3}[A-Z]{3}) rep(\d+)',mapping_analysis['name'])
    if m:
        repn = int(m.group(2))
    else:
        logger.error("Missing rep in %s, skipping." %(mapping_analysis['name']))
        return []

    logger.info("%s rep %d: accessioning mapping." %(experiment_accession, repn))

    experiment = common.encoded_get(urlparse.urljoin(server,'/experiments/%s' %(experiment_accession)), keypair)
    raw_mapping_stages = get_raw_mapping_stages(mapping_analysis, keypair, server, repn)

    output_files = accession_outputs(raw_mapping_stages, experiment, keypair, server, dryrun, force)

    files_with_derived = patch_outputs(raw_mapping_stages, keypair, server, dryrun)

    raw_mapping_analysis_step_versions = {
        'bwa-indexing-step-v-1' : [
            {
                'stages' : "",
                'stage_name': "",
                'file_names' : [],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'bwa-raw-alignment-step-v-1' : [
            {
                'stages' : raw_mapping_stages,
                'stage_name': 'Map ENCSR*',
                'file_names' : ['mapped_reads'],
                'status' : 'finished',
                'qc_objects' : []
            }
        ]
    }

    patched_files = accession_pipeline(raw_mapping_analysis_step_versions, keypair, server, dryrun, force)
    return patched_files

def accession_histone_analysis_files(peaks_analysis, keypair, server, dryrun, force):

    # m = re.match('^(ENCSR[0-9]{3}[A-Z]{3}) Peaks',peaks_analysis['executableName'])
    # if m:
    #   experiment_accession = m.group(1)
    #   logger.info(experiment_accession)
    experiment_accession = get_experiment_accession(peaks_analysis)

    if experiment_accession:
        logger.info('%s: accession histone peaks' %(experiment_accession))
    else:
        logger.error("No experiment accession in %s, skipping." %(peaks_analysis['executableName']))
        return None

    #returns the experiment object
    experiment = common.encoded_get(urlparse.urljoin(server,'/experiments/%s' %(experiment_accession)), keypair)

    #returns a list with two elements:  the mapping stages for [rep1,rep2]
    #in this context rep1,rep2 are the first and second replicates in the pipeline.  They may have been accessioned
    #on the portal with any arbitrary biological_replicate_numbers.
    mapping_stages = get_peak_mapping_stages(peaks_analysis, experiment, keypair, server)
    if not mapping_stages:
        logger.error("Failed to find peak mapping stages")
        return None

    #returns a list with three elements: the mapping stages for the controls for [rep1, rep2, pooled]
    #the control stages for rep1 and rep2 might be the same as the pool if the experiment used pooled controls
    control_stages = get_control_mapping_stages(peaks_analysis, experiment, keypair, server)
    if not control_stages:
        logger.error("Failed to find control mapping stages")
        return None

    #returns the stages for peak calling
    peak_stages = get_histone_peak_stages(peaks_analysis, mapping_stages, control_stages, experiment, keypair, server)
    if not peak_stages:
        logger.error("Failed to find peak stages")
        return None

    #accession all the output files
    output_files = []
    for stages in [control_stages[0], control_stages[1], mapping_stages[0], mapping_stages[1], peak_stages]:
        logger.info('accessioning output')
        output_files.extend(accession_outputs(stages, experiment, keypair, server, dryrun, force))

    #now that we have file accessions, loop again and patch derived_from
    files_with_derived = []
    for stages in [control_stages[0], control_stages[1], mapping_stages[0], mapping_stages[1], peak_stages]:
        files_with_derived.extend(patch_outputs(stages, keypair, server, dryrun))

    full_analysis_step_versions = {
        'bwa-indexing-step-v-1' : [
            {
                'stages' : "",
                'stage_name': "",
                'file_names' : [],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'bwa-alignment-step-v-1' : [
            {
                'stages' : control_stages[0],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}

                ]
            },
            {
                'stages' : control_stages[1],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}
                ]
            },
            {
                'stages' : mapping_stages[0],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}
                ]
            },
            {
                'stages' : mapping_stages[1],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}
                ]
            }           
        ],
        'histone-peak-calling-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'ENCODE Peaks',
                'file_names' : ['rep1_fc_signal', 'rep2_fc_signal', 'pooled_fc_signal', 'rep1_pvalue_signal', 'rep2_pvalue_signal', 'pooled_pvalue_signal', 'rep1_narrowpeaks', 'rep2_narrowpeaks', 'pooled_narrowpeaks'],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'histone-overlap-peaks-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'Overlap narrowpeaks',
                'file_names' : ['overlapping_peaks'],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'histone-peaks-to-bigbed-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'ENCODE Peaks',
                'file_names' : ['rep1_narrowpeaks_bb', 'rep2_narrowpeaks_bb', 'pooled_narrowpeaks_bb'],
                'status' : 'virtual',
                'qc_objects': []
            }
        ],
        'histone-replicated-peaks-to-bigbed-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'Overlap narrowpeaks',
                'file_names' : ['overlapping_peaks_bb'],
                'status' : 'virtual',
                'qc_objects': []
            }
        ]
    }

    patched_files = accession_pipeline(full_analysis_step_versions, keypair, server, dryrun, force)
    return patched_files

def accession_tf_analysis_files(peaks_analysis, keypair, server, dryrun, force):

    # m = re.match('^(ENCSR[0-9]{3}[A-Z]{3}) Peaks',peaks_analysis['executableName'])
    # if m:
    #   experiment_accession = m.group(1)
    #   logger.info(experiment_accession)
    experiment_accession = get_experiment_accession(peaks_analysis)

    if experiment_accession:
        logger.info('%s: accession TF peaks' %(experiment_accession))
    else:
        logger.error("No experiment accession in %s, skipping." %(peaks_analysis['executableName']))
        return None

    #returns the experiment object
    experiment = common.encoded_get(urlparse.urljoin(server,'/experiments/%s' %(experiment_accession)), keypair)
    logger.debug('got experiment %s' %(experiment.get('accession')))
    #returns a list with two elements:  the mapping stages for [rep1,rep2]
    #in this context rep1,rep2 are the first and second replicates in the pipeline.  They may have been accessioned
    #on the portal with any arbitrary biological_replicate_numbers.
    mapping_stages = get_peak_mapping_stages(peaks_analysis, experiment, keypair, server)
    if not mapping_stages:
        logger.error("Failed to find peak mapping stages")
        return None

    #returns a list with three elements: the mapping stages for the controls for [rep1, rep2, pooled]
    #the control stages for rep1 and rep2 might be the same as the pool if the experiment used pooled controls
    control_stages = get_control_mapping_stages(peaks_analysis, experiment, keypair, server)
    if not control_stages:
        logger.error("Failed to find control mapping stages")
        return None

    #returns the stages for peak calling
    peak_stages = get_tf_peak_stages(peaks_analysis, mapping_stages, control_stages, experiment, keypair, server)
    if not peak_stages:
        logger.error("Failed to find peak stages")
        return None

    #accession all the output files
    output_files = []
    for stages in [control_stages[0], control_stages[1], mapping_stages[0], mapping_stages[1], peak_stages]:
        logger.info('accessioning output')
        output_files.extend(accession_outputs(stages, experiment, keypair, server, dryrun, force))

    #now that we have file accessions, loop again and patch derived_from
    files_with_derived = []
    for stages in [control_stages[0], control_stages[1], mapping_stages[0], mapping_stages[1], peak_stages]:
        files_with_derived.extend(patch_outputs(stages, keypair, server, dryrun))

    full_analysis_step_versions = {
        'bwa-indexing-step-v-1' : [
            {
                'stages' : "",
                'stage_name': "",
                'file_names' : [],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'bwa-alignment-step-v-1' : [
            {
                'stages' : control_stages[0],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}

                ]
            },
            {
                'stages' : control_stages[1],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}
                ]
            },
            {
                'stages' : mapping_stages[0],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}
                ]
            },
            {
                'stages' : mapping_stages[1],
                'stage_name': 'Filter and QC*',
                'file_names' : ['filtered_bam'],
                'status' : 'finished',
                'qc_objects' : [
                    {'chipseq_filter_quality_metric': ['filtered_bam']},
                    {'samtools_flagstats_quality_metric': ['filtered_bam']}
                ]
            }           
        ],
        'tf-spp-peak-calling-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'SPP Peaks',
                'file_names' : ['rep1_peaks', 'rep2_peaks', 'pooled_peaks'],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'tf-macs2-signal-calling-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'ENCODE Peaks',
                'file_names' : ['rep1_fc_signal', 'rep2_fc_signal', 'pooled_fc_signal', 'rep1_pvalue_signal', 'rep2_pvalue_signal', 'pooled_pvalue_signal'],
                'status' : 'finished',
                'qc_objects': []
            }
        ],
        'tf-idr-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'Final IDR peak calls',
                'file_names' : ['conservative_set','optimal_set'],
                'status' : 'finished',
                'qc_objects': [
                    {'idr_quality_metric': ['conservative_set','optimal_set']}
                ]
            }
        ],
        'tf-peaks-to-bigbed-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'SPP Peaks',
                'file_names' : ['rep1_peaks_bb', 'rep2_peaks_bb', 'pooled_peaks_bb'],
                'status' : 'virtual',
                'qc_objects': []
            }
        ],
        'tf-idr-peaks-to-bigbed-step-v-1' : [
            {
                'stages' : peak_stages,
                'stage_name': 'Final IDR peak calls',
                'file_names' : ['conservative_set_bb','optimal_set_bb'],
                'status' : 'virtual',
                'qc_objects': [
                    {'idr_quality_metric': ['conservative_set_bb','optimal_set_bb']}
                ]
            }
        ]
    }

    patched_files = accession_pipeline(full_analysis_step_versions, keypair, server, dryrun, force)
    return patched_files

@dxpy.entry_point('main')
def main(outfn, assembly, debug, key, keyfile, dryrun, force, pipeline=None, analysis_ids=None, infile=None, project=None):

    if debug:
        logger.info('setting logger level to logging.DEBUG')
        logger.setLevel(logging.DEBUG)
    else:
        logger.info('setting logger level to logging.INFO')
        logger.setLevel(logging.INFO)

    if infile is not None:
        infile = dxpy.DXFile(infile)
        dxpy.download_dxfile(infile.get_id(), "infile")
        ids = open("infile",'r')
    elif analysis_ids is not None:
        ids = analysis_ids
    else:
        logger.error("Must supply one of --infile or a list of one or more analysis-ids")
        return

    authid, authpw, server = common.processkey(key, keyfile)
    keypair = (authid,authpw)

    common_metadata.update({'assembly': assembly})

    with open(outfn, 'w') as fh:
        if dryrun:
            fh.write('---DRYRUN: No files have been modified---\n')
        fieldnames = ['analysis','experiment','assembly','dx_pipeline','files','error']
        output_writer = csv.DictWriter(fh, fieldnames, delimiter='\t')
        output_writer.writeheader()

        for (i, analysis_id) in enumerate(ids):
            logger.debug('debug %s' %(analysis_id))
            analysis = dxpy.describe(analysis_id.strip())
            experiment = get_experiment_accession(analysis)
            output = {
                'analysis': analysis_id,
                'experiment': experiment,
                'assembly': assembly
            }
            logger.info('Accessioning analysis name %s executableName %s' %(analysis.get('name'), analysis.get('executableName')))

            if pipeline == "histone" or analysis.get('name') == 'histone_chip_seq':
                output.update({'dx_pipeline':'histone_chip_seq'})
                accessioned_files = accession_histone_analysis_files(analysis, keypair, server, dryrun, force)
                logger.info('accession histone analysis completed')
            elif pipeline == "mapping" or analysis.get('executableName') == 'ENCODE mapping pipeline':
                output.update({'dx_pipeline':'ENCODE mapping pipeline'})
                accessioned_files = accession_mapping_analysis_files(analysis, keypair, server, dryrun, force)
                logger.info('accession mapping analysis completed')
            elif pipeline == "tf" or analysis.get('executableName') == 'tf_chip_seq':
                output.update({'dx_pipeline':'tf_chip_seq'})
                accessioned_files = accession_tf_analysis_files(analysis, keypair, server, dryrun, force)
                logger.info('accession tf_chip_seq analysis completed')
            elif pipeline == "raw":
                output.update({'dx_pipeline':'ENCODE raw mapping pipeline'})
                accessioned_files = accession_raw_mapping_analysis_files(analysis, keypair, server, dryrun, force)
                logger.info('accession raw mapping analysis completed')
            else:
                logger.error('unrecognized analysis pattern %s %s ... skipping.' %(analysis.get('name'), analysis.get('executableName')))
                output.update({'dx_pipeline':'unrecognized'})
                accessioned_files = None

            file_accessions = [f.get('accession') for f in (accessioned_files or [])]
            logger.info("Accessioned: %s" %(file_accessions))
            output.update({'files':file_accessions})
            output_writer.writerow(output)

            if file_accessions:
                url = server+"/experiments/%s" %(experiment)
                r = common.encoded_patch(url,keypair,{"internal_status":"pipeline completed"},return_response=True)
                try:
                    r.raise_for_status()
                except:
                    logger.error("Tried but failed to update experiment internal_status to pipeline completed")
                    logger.error(r.text)

    common.touch(outfn)
    outfile = dxpy.upload_local_file(outfn)

    output = {}
    output["outfile"] = dxpy.dxlink(outfile)

    return output

dxpy.run()
